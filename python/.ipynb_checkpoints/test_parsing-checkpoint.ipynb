{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import feather\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# Read data file into a pandas dataframe\n",
    "read_df = feather.read_dataframe('../parsed_data/filtered_tip_data.feather', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = read_df.head(300)\n",
    "# type(bus_df.stars[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions for normalising text data\n",
    "\n",
    "# Convert all words to lowercase, remove punctuation, tokenise and stem\n",
    "# and remove stopwords, threshold = 10%\n",
    "def norm_corpus(document):\n",
    "    \n",
    "    # unicode decode\n",
    "    document = document.decode('utf-8')\n",
    "    \n",
    "    # lowercase and remove symbols\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc_tokens = tokenizer.tokenize(document.lower())\n",
    "        \n",
    "    # remove stopwords\n",
    "    doc_tokens = [word for word in doc_tokens if word not in stopwords.words('english')]\n",
    "        \n",
    "    # stem words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    doc_stem = [stemmer.stem(word) for word in doc_tokens]\n",
    "        \n",
    "    # make tokenised text one string\n",
    "    norm_doc = \" \".join(doc_stem)\n",
    "    \n",
    "    return norm_doc\n",
    "\n",
    "\n",
    "# Vectorise keywords from normalised text to vector including only nouns and adjectives\n",
    "def review_vector(norm_doc):\n",
    "\n",
    "    # select all words categorised as nouns or adjectives\n",
    "    # loop through each string i.e. review in the df column\n",
    "    review_keyword_list = []\n",
    "    doc = nltk.word_tokenize(norm_doc)\n",
    "\n",
    "    # create tuple for each word in list: (word, tag)\n",
    "    token_category = nltk.pos_tag(doc)  \n",
    "\n",
    "    for word, tag in token_category:   \n",
    "            \n",
    "        # nouns\n",
    "        if (tag == 'NN' or tag == 'NNS' or tag == 'NNP' or tag == 'NNPS'):\n",
    "            review_keyword_list.append(word)\n",
    "                \n",
    "        # adjectives\n",
    "        elif (tag == 'JJ' or tag == 'JJS' or tag == 'JJP' or tag == 'JJPS'):\n",
    "            review_keyword_list.append(word)\n",
    "        \n",
    "        else:\n",
    "            pass     \n",
    "        \n",
    "    review_keywords = \" \".join(review_keyword_list)\n",
    "        \n",
    "    # vectorise string\n",
    "    WORD = re.compile(r'\\w+')\n",
    "    review_vector = Counter(WORD.findall(review_keywords))\n",
    "    \n",
    "    \n",
    "    return review_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business_id',\n",
       " 'date',\n",
       " 'likes',\n",
       " 'text',\n",
       " 'type',\n",
       " 'user_id',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'name',\n",
       " 'city',\n",
       " 'stars',\n",
       " 'review_count',\n",
       " 'food_drink']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip text normalised, next: vectorise\n"
     ]
    }
   ],
   "source": [
    "## Tip data\n",
    "output_df = test.ix[:,['business_id', 'date', 'likes', 'text', 'type',\n",
    "                       'user_id', 'latitude', 'longitude', 'name', 'city',\n",
    "                       'stars', 'review_count', 'food_drink']]\n",
    "output_df.text = test.text.apply(lambda x: norm_corpus(x))\n",
    "print \"tip text normalised, next: vectorise\"\n",
    "output_df.text = output_df.text.apply(lambda x: review_vector(x))\n",
    "\n",
    "## Review data\n",
    "# bus_df = bus_df[bus_df.stars > 3]\n",
    "# output_df = bus_df.ix[:,['business_id', 'user_id', 'date', 'stars', 'votes', 'text']]\n",
    "# output_df.text = output_df.text.apply(lambda x: norm_corpus(x))\n",
    "# print \"tip text normalised, next: vectorise\"\n",
    "# output_df.text = output_df.text.apply(lambda x: review_vector(x))\n",
    "# output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-04-18</th>\n",
       "      <td>cE27W9VPgO88Qxe4ol6y_g</td>\n",
       "      <td>-6rEfobYjMxpUWLNxszaxQ</td>\n",
       "      <td>2013-04-18</td>\n",
       "      <td>{u'wast': 1, u'time': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>mVHrayjG3uZ_RLHkLj-AMg</td>\n",
       "      <td>EZ0r9dKKtEGVx2CdnowPCw</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>{u'depart': 1, u'polic': 1, u'rankin': 1, u'di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-03</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>xb6zEQCw9I-Gl0g06e1KsQ</td>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>{u'great': 1, u'special': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-08</th>\n",
       "      <td>KayYbHCt-RkbGcPdGOThNg</td>\n",
       "      <td>QawZN4PSW7ng_9SP7pjsVQ</td>\n",
       "      <td>2015-07-08</td>\n",
       "      <td>{u'great': 1, u'good': 1, u'food': 1, u'beer':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-25</th>\n",
       "      <td>1_lU0-eSWJCRvNGk78Zh9Q</td>\n",
       "      <td>MLQre1nvUtW-RqMTc4iC9A</td>\n",
       "      <td>2015-10-25</td>\n",
       "      <td>{u'beauti': 1, u'restor': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       business_id                 user_id       date  \\\n",
       "date                                                                    \n",
       "2013-04-18  cE27W9VPgO88Qxe4ol6y_g  -6rEfobYjMxpUWLNxszaxQ 2013-04-18   \n",
       "2013-01-06  mVHrayjG3uZ_RLHkLj-AMg  EZ0r9dKKtEGVx2CdnowPCw 2013-01-06   \n",
       "2013-12-03  KayYbHCt-RkbGcPdGOThNg  xb6zEQCw9I-Gl0g06e1KsQ 2013-12-03   \n",
       "2015-07-08  KayYbHCt-RkbGcPdGOThNg  QawZN4PSW7ng_9SP7pjsVQ 2015-07-08   \n",
       "2015-10-25  1_lU0-eSWJCRvNGk78Zh9Q  MLQre1nvUtW-RqMTc4iC9A 2015-10-25   \n",
       "\n",
       "                                                         text  \n",
       "date                                                           \n",
       "2013-04-18                           {u'wast': 1, u'time': 1}  \n",
       "2013-01-06  {u'depart': 1, u'polic': 1, u'rankin': 1, u'di...  \n",
       "2013-12-03                       {u'great': 1, u'special': 1}  \n",
       "2015-07-08  {u'great': 1, u'good': 1, u'food': 1, u'beer':...  \n",
       "2015-10-25                       {u'beauti': 1, u'restor': 1}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Group by same month in year to count number of emails per topic\n",
    "## i.e. -> emails sent per topic monthly\n",
    "# make date column index\n",
    "output_df.index = pd.to_datetime(output_df.date, \n",
    "                                 format='%m/%d/%Y')\n",
    "\n",
    "# perform groupby, summinng up dummies for count\n",
    "monthly = pd.DataFrame.groupby(output_df, \n",
    "                               by=[output_df.index.year, \n",
    "                                   output_df.index.month]).aggregate(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
